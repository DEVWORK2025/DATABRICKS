{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2d48adbb-148b-4818-a2e2-5715a0919470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# UDF\n",
    "\n",
    "User defined functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef96bdd-e3c8-4b69-994a-7632bb4e6d30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+\n| id|  name|salary|bonus|\n+---+------+------+-----+\n|  1|Nilesh| 55000|    3|\n|  2|  Nita| 90000|    5|\n+---+------+------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'Nilesh','55000','3'),(2,'Nita','90000','5')]\n",
    "columns =['id','name','salary','bonus']\n",
    "df = spark.createDataFrame(data = data1, schema= columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677d223c-e0b2-4b35-8af2-654101ff85cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[7]: 10"
     ]
    }
   ],
   "source": [
    "a=10\n",
    "int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61b1eb0b-edcb-4d4f-b1e7-3be14dcbb16c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "def total_sal(sal, com):\n",
    "    try:\n",
    "        out=int(sal)+(float(sal)*(float(com)/100))\n",
    "    except:\n",
    "        out = sys.exc_info()[1]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a58778d-acfd-4174-adbf-1cf9db5ff8d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: 3090.0"
     ]
    }
   ],
   "source": [
    "total_sal('3000','3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbb6e5e8-bedb-467a-8b1d-6d9e13dc9e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "\n",
    "commission_calc = udf(lambda sal, com : total_sal(sal, com), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1324e0cf-54ea-468b-b3b0-695efa527f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+-------+\n| id|  name|salary|bonus|    COM|\n+---+------+------+-----+-------+\n|  1|Nilesh| 55000|    3|56650.0|\n|  2|  Nita| 90000|    5|94500.0|\n+---+------+------+-----+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('COM', commission_calc(df.salary, df.bonus) ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1218643-d5fc-49e2-be4e-6e4790f76f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## instead of doing all the operations we may use decorator here\n",
    "import sys\n",
    "@udf(returnType = FloatType())\n",
    "def total_sal1(sal, com):\n",
    "    try:\n",
    "        out=int(sal)+(float(sal)*(float(com)/100))\n",
    "    except:\n",
    "        out = sys.exc_info()[1]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f606ac-8360-442b-b316-d72da3369275",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: Column<'total_sal1(3000, 3)'>"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "total_sal1(lit(3000),lit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197d0e0b-3e07-43f3-a011-fb1ee5d0bbb9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-----+----------+\n| id|  name|salary|bonus|commission|\n+---+------+------+-----+----------+\n|  1|Nilesh| 55000|    3|   56650.0|\n|  2|  Nita| 90000|    5|   94500.0|\n+---+------+------+-----+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select('*', total_sal1(df.salary, df.bonus).alias('commission')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "389ecb67-8aec-4e76-8de8-d43085093b8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### creating the udf function and using the same as SQL function\n",
    "df.createOrReplaceTempView('dummy_emp_tbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "803b4580-0b74-4b8a-9852-b3745d2f3fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def total_sal2(sal, com):\n",
    "    try:\n",
    "        out=int(sal)+(float(sal)*(float(com)/100))\n",
    "    except:\n",
    "        out = sys.exc_info()[1]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28408b8b-c7b3-43ec-a267-8f614b33e0a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: <function __main__.total_sal2(sal, com)>"
     ]
    }
   ],
   "source": [
    "spark.udf.register(name = 'cal_com', f=total_sal2, returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aa9acb8-b256-4782-9e0e-7d93d304bd90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.udf.register(name = 'cal_com', f=total_sal2, returnType=FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fa4f7f5-c619-4fc5-97d8-6f2cda9b4726",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>salary</th><th>bonus</th><th>cal_com(salary, bonus)</th></tr></thead><tbody><tr><td>55000</td><td>3</td><td>56650.0</td></tr><tr><td>90000</td><td>5</td><td>94500.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "55000",
         "3",
         56650.0
        ],
        [
         "90000",
         "5",
         94500.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "bonus",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "cal_com(salary, bonus)",
         "type": "\"float\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "select * from dummy_emp_tbl;\n",
    "\n",
    "select salary, bonus, cal_Com(salary, bonus)\n",
    "from dummy_emp_tbl;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3d123202-1132-4ab4-bc92-1ad8acac5594",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## RDD\n",
    "\n",
    "Resilient Distributed Dataset\n",
    "\n",
    "it's immutable, and data is in memmory processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b2a3a9-fc92-47d9-aa23-fc316f2c90fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[10] at readRDDFromInputStream at PythonRDD.scala:435\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'Nilesh','55000','3'),(2,'Nita','90000','5')]\n",
    "rdd = spark.sparkContext.parallelize(data1)\n",
    "print(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e444813d-effc-4d14-a7b4-ae1a4117dfa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "## collect the data from different nodes and partition\n",
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "345e5a12-aca6-439c-92a7-099598dac49e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 'Nilesh', '55000', '3'), (2, 'Nita', '90000', '5')]\n"
     ]
    }
   ],
   "source": [
    "print(rdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a69bfc-af4d-41d1-839a-00c1ae4ae223",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+--------+\n| id|  name|salary|interest|\n+---+------+------+--------+\n|  1|Nilesh| 55000|       3|\n|  2|  Nita| 90000|       5|\n+---+------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "### \n",
    "df1 = rdd.toDF(['id','name','salary','interest'])\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f16e8b70-a101-4a27-8125-58d67cab61ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+--------+\n| id|  name|salary|interest|\n+---+------+------+--------+\n|  1|Nilesh| 55000|       3|\n|  2|  Nita| 90000|       5|\n+---+------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = spark.createDataFrame(rdd,['id','name','salary','interest'] )\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce4c2d7-d9fa-406a-a6e8-5b8b7988f044",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[12] at readRDDFromInputStream at PythonRDD.scala:435\n"
     ]
    }
   ],
   "source": [
    "data1 = [('Nilesh','Chavan'),('Nita','Kale')]\n",
    "rdd1 = spark.sparkContext.parallelize(data1)\n",
    "print(rdd1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2972a7c8-f4ac-406a-a02c-ae2d10a0f66d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n|  col1|  col2|\n+------+------+\n|Nilesh|Chavan|\n|  Nita|  Kale|\n+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df3 = spark.createDataFrame(rdd1,['col1','col2'] )\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7a33de1-d14e-4b85-8794-5a3647c1930b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[32]: [('Nilesh', 'Chavan'), ('Nita', 'Kale')]"
     ]
    }
   ],
   "source": [
    "tobj=rdd1.collect()\n",
    "tobj[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f835d46d-ce74-4f24-88cb-9db96ef2fb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Nilesh', 'Chavan', 'Nilesh Chavan'), ('Nita', 'Kale', 'Nita Kale')]\n+------+------+-------------+\n|f_name|l_name|    full_name|\n+------+------+-------------+\n|Nilesh|Chavan|Nilesh Chavan|\n|  Nita|  Kale|    Nita Kale|\n+------+------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "rdd2 = rdd1.map(lambda x: x+ (x[0]+' '+x[1],))\n",
    "print(rdd2.collect())\n",
    "df3 = rdd2.toDF(['f_name','l_name','full_name'])\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7a46cff-3194-403a-8e2c-7de18d759fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr. Anay\nMiss Anita\n"
     ]
    }
   ],
   "source": [
    "data123 = ['Mr. Anay','Miss Anita']\n",
    "rdd = spark.sparkContext.parallelize(data123)\n",
    "\n",
    "for i in rdd.collect():\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b79e18a-1e29-436f-bd39-966b8da87c36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mr.', 'Anay']\n['Miss', 'Anita']\n"
     ]
    }
   ],
   "source": [
    "rdd123 = rdd.map(lambda x: x.split(' '))\n",
    "for i in rdd123.collect():\n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0967a12-b35e-4054-814a-74274dca6be9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr.\nAnay\nMiss\nAnita\n"
     ]
    }
   ],
   "source": [
    "rdd123 = rdd.flatMap(lambda x: x.split(' '))\n",
    "for i in rdd123.collect():\n",
    "    print(i) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cb062a1-9d21-4dfc-addc-4248ee66474a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## PartitionBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4b1aa3f-e371-4454-baa1-d7eb939dc0e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+---------+-----+\n| Month|   TV|radio|newspaper|sales|\n+------+-----+-----+---------+-----+\n|Jan-00|230.1| 37.8|     69.2| 22.1|\n|Feb-00| 44.5| 39.3|     45.1| 10.4|\n|Mar-00| 17.2| 45.9|     69.3|  9.3|\n|Apr-00|151.5| 41.3|     58.5| 18.5|\n|May-00|180.8| 10.8|     58.4| 12.9|\n|Jun-00|  8.7| 48.9|     75.0|  7.2|\n|Jul-00| 57.5| 32.8|     23.5| 11.8|\n|Aug-00|120.2| 19.6|     11.6| 13.2|\n|Sep-00|  8.6|  2.1|      1.0|  4.8|\n|Oct-00|199.8|  2.6|     21.2| 10.6|\n|Nov-00| 66.1|  5.8|     24.2|  8.6|\n|Dec-00|214.7| 24.0|      4.0| 17.4|\n|01-Jan| 23.8| 35.1|     65.9|  9.2|\n|01-Feb| 97.5|  7.6|      7.2|  9.7|\n|01-Mar|204.1| 32.9|     46.0| 19.0|\n|01-Apr|195.4| 47.7|     52.9| 22.4|\n|01-May| 67.8| 36.6|    114.0| 12.5|\n|01-Jun|281.4| 39.6|     55.8| 24.4|\n|01-Jul| 69.2| 20.5|     18.3| 11.3|\n|01-Aug|147.3| 23.9|     19.1| 14.6|\n+------+-----+-----+---------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "adv_df = spark.read.format('csv').option('header','true').option('inferSchema', 'true')\\\n",
    "    .load('dbfs:/FileStore/Advertising.csv')\n",
    "adv_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "064c5d0b-9719-4700-bdea-d973a17abaec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "adv_df.write.parquet(path = 'dbfs:/temps/work/', mode= 'overwrite', partitionBy = 'TV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02cf09b1-383b-4110-8b5b-d61809df5c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+-----+-----+\n| Month|radio|newspaper|sales|   TV|\n+------+-----+---------+-----+-----+\n|Oct-00|  2.6|     21.2| 10.6|199.8|\n|04-Mar|  3.1|     34.6| 11.4|199.8|\n|08-Feb| 21.0|     22.0| 15.5|184.9|\n|11-Aug| 43.9|      1.7| 20.7|184.9|\n|08-May|  4.3|     49.8| 11.7|222.4|\n|14-Jul|  3.4|     13.1| 11.5|222.4|\n|01-Oct|  5.1|     23.5| 12.5|237.4|\n|05-Sep| 27.5|     11.0| 18.9|237.4|\n|05-Dec| 14.3|     31.7| 12.4|109.8|\n|07-Jun| 47.8|     51.4| 16.7|109.8|\n|08-Jan|  3.5|      5.9| 11.7|197.6|\n|12-Sep| 23.3|     14.2| 16.6|197.6|\n|03-Jun| 33.4|     38.7| 17.1|177.0|\n|16-Jun|  9.3|      6.4| 12.8|177.0|\n|06-Sep| 26.7|     22.3| 11.8| 76.4|\n|09-Oct|  0.8|     14.8|  9.4| 76.4|\n|Mar-00| 45.9|     69.3|  9.3| 17.2|\n|16-Jan|  4.1|     31.6|  5.9| 17.2|\n|02-Apr| 16.7|     22.9| 15.9|240.1|\n|12-Mar|  7.3|      8.7| 13.2|240.1|\n+------+-----+---------+-----+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "### read all folders created in parquet file\n",
    "spark.read.parquet('dbfs:/temps/work/').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1996abf9-a6fd-4753-a1ba-43423acda74e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+---------+-----+\n| Month|radio|newspaper|sales|\n+------+-----+---------+-----+\n|06-Jun| 28.5|     14.2| 14.2|\n+------+-----+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet('dbfs:/temps/work/TV=120.5').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64538a2d-17ab-427f-be63-ca493144ca0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "adv_df.write.parquet(path = 'dbfs:/temps/work/', mode= 'overwrite', partitionBy = ['TV','radio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0f531587-bff2-4f29-891d-367e8a565458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Json type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4b851c8-2c23-4657-8bf3-44cc94af14ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------+\n|name  |games                              |\n+------+-----------------------------------+\n|Sunita|{'g1': 'football', 'g2': 'Archary'}|\n|Fatima|{'g1': 'Baseball', 'g2': 'chess'}  |\n+------+-----------------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- games: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [('Sunita', \"{'g1': 'football', 'g2': 'Archary'}\"),('Fatima',\"{'g1': 'Baseball', 'g2': 'chess'}\")]\n",
    "columns =['name', 'games']\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "df.show(truncate = False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f7c14df-15dd-4b69-8389-097100af5e87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "convert the json string to map type\n",
    "\n",
    "to_json is the function to convert any map type or struct type  to json string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f181f844-759a-4a05-8541-f1b2506ef098",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------+-------------------------------+\n|name  |games                              |dict_map                       |\n+------+-----------------------------------+-------------------------------+\n|Sunita|{'g1': 'football', 'g2': 'Archary'}|{g1 -> football, g2 -> Archary}|\n|Fatima|{'g1': 'Baseball', 'g2': 'chess'}  |{g1 -> Baseball, g2 -> chess}  |\n+------+-----------------------------------+-------------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- games: string (nullable = true)\n |-- dict_map: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import MapType, StringType\n",
    "\n",
    "MapTypeStructure= MapType(StringType(), StringType() )\n",
    "\n",
    "df1 = df.withColumn('dict_map', from_json(df.games,  MapTypeStructure))\n",
    "df1.show(truncate = False)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5983f4e8-d6a0-4c0e-846a-96efd853b5c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df1.withColumn('g1', df1.dict_map.g1).withColumn('g2',df1.dict_map.g2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79044c32-0bfd-4e12-94da-5ebab25d660d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------+-------------------------------+--------+-------+\n|name  |games                              |dict_map                       |g1      |g2     |\n+------+-----------------------------------+-------------------------------+--------+-------+\n|Sunita|{'g1': 'football', 'g2': 'Archary'}|{g1 -> football, g2 -> Archary}|football|Archary|\n|Fatima|{'g1': 'Baseball', 'g2': 'chess'}  |{g1 -> Baseball, g2 -> chess}  |Baseball|chess  |\n+------+-----------------------------------+-------------------------------+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show(truncate= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "540ccebe-1925-4676-bfad-2526abc2df1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Struct Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d84ed86d-acc5-4c4f-80f9-667b436959e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- games: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aa5243c-1e28-4ddf-9b0c-41bb07ae048c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------+-------------------+\n|name  |games                              |game_struct        |\n+------+-----------------------------------+-------------------+\n|Sunita|{'g1': 'football', 'g2': 'Archary'}|{football, Archary}|\n|Fatima|{'g1': 'Baseball', 'g2': 'chess'}  |{Baseball, chess}  |\n+------+-----------------------------------+-------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- games: string (nullable = true)\n |-- game_struct: struct (nullable = true)\n |    |-- g1: string (nullable = true)\n |    |-- g2: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import StructType, StructField , StringType\n",
    "\n",
    "schema1 = StructType([ StructField('g1',StringType()),\\\n",
    "                    StructField('g2',StringType())])\n",
    "\n",
    "df1 = df.withColumn('game_struct', from_json(df.games, schema1 ))\n",
    "df1.show(truncate = False)\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e29b521-7aa2-4286-a58e-7a929eeafc69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+--------+\n|  name|               games|        game_struct|   game1|\n+------+--------------------+-------------------+--------+\n|Sunita|{'g1': 'football'...|{football, Archary}|football|\n|Fatima|{'g1': 'Baseball'...|  {Baseball, chess}|Baseball|\n+------+--------------------+-------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn('game1', df1.game_struct.g1 )\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91258741-38bc-47b7-8f5b-8ed9e99f6f49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------+\n|name  |games                          |\n+------+-------------------------------+\n|Sunita|{g1 -> football, g2 -> Archary}|\n|Fatima|{g1 -> Baseball, g2 -> chess}  |\n+------+-------------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- games: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [('Sunita', {'g1': 'football', 'g2': 'Archary'}),('Fatima',{'g1': 'Baseball', 'g2': 'chess'})]\n",
    "columns =['name', 'games']\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "df.show(truncate = False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de814e88-8fd1-4fec-8645-00110de672b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n|  name|               games|           game_dict|\n+------+--------------------+--------------------+\n|Sunita|{g1 -> football, ...|{\"g1\":\"football\",...|\n|Fatima|{g1 -> Baseball, ...|{\"g1\":\"Baseball\",...|\n+------+--------------------+--------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- games: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- game_dict: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_json\n",
    "df1 = df.withColumn('game_dict', to_json(df.games))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65330d4b-7d78-4805-9941-8eb17577ebc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[10]: [('Sunita', {'g1': 'football', 'g2': 'Archary'}),\n ('Fatima', {'g1': 'Baseball', 'g2': 'chess'})]"
     ]
    }
   ],
   "source": [
    "data1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e3c8357-c8f9-4bdd-afae-1f2f29f09336",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------+\n|name  |games                          |\n+------+-------------------------------+\n|Sunita|{g1 -> football, g2 -> Archary}|\n|Fatima|{g1 -> Baseball, g2 -> chess}  |\n+------+-------------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- games: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2950f92-b6e6-444f-bf4c-535cc6bbb52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Json tuple\n",
    "to extract few keys from the given string in the form of dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44aaa22e-0c93-4ea3-94cd-60cf19c2eea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------------------+\n|name  |games                              |\n+------+-----------------------------------+\n|Sunita|{'g1': 'football', 'g2': 'Archary'}|\n|Fatima|{'g1': 'Baseball', 'g2': 'chess'}  |\n+------+-----------------------------------+\n\nroot\n |-- name: string (nullable = true)\n |-- games: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import json_tuple\n",
    "data1 = [('Sunita', \"{'g1': 'football', 'g2': 'Archary'}\"),('Fatima',\"{'g1': 'Baseball', 'g2': 'chess'}\")]\n",
    "columns =['name', 'games']\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "df.show(truncate = False)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9605b547-ce27-4568-af9b-c9cf3b00ef63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n|  name|      g1|     g2|\n+------+--------+-------+\n|Sunita|football|Archary|\n|Fatima|Baseball|  chess|\n+------+--------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df2= df.select(df.name, json_tuple(df.games,'g1','g2').alias('g1','g2'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36dfa324-5f21-45f6-af8e-b075d2362e72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## get json_objects\n",
    "\n",
    "get the information from nested dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a6c2a93-597e-4328-af6d-a2dce65120c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------------------------------------+\n|Emp_id|pdetails                                                                              |\n+------+--------------------------------------------------------------------------------------+\n|EMP100|{'name': 'Anil', 'address':{'city':'Solapur', 'landmark':'Navi Peth'}, 'gender': 'M'} |\n|EMP101|{'name':'Sunil', 'address':{'city':'Kolhapur', 'landmark':'Juni Peth'}, 'gender': 'M'}|\n|EMP102|{'name': 'Neel', 'address':{'city':'Nagpur', 'landmark':'Peth'}, 'gender': 'M'}       |\n+------+--------------------------------------------------------------------------------------+\n\nroot\n |-- Emp_id: string (nullable = true)\n |-- pdetails: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [('EMP100', \"{'name': 'Anil', 'address':{'city':'Solapur', 'landmark':'Navi Peth'}, 'gender': 'M'}\"),\n",
    "        ('EMP101', \"{'name':'Sunil', 'address':{'city':'Kolhapur', 'landmark':'Juni Peth'}, 'gender': 'M'}\"),\n",
    "        ('EMP102', \"{'name': 'Neel', 'address':{'city':'Nagpur', 'landmark':'Peth'}, 'gender': 'M'}\")]\n",
    "\n",
    "schema = ['Emp_id', 'pdetails']\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show(truncate = False)\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e2dedd5-8986-47c6-9434-5cc9ca385007",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n|Emp_id|    city|\n+------+--------+\n|EMP100| Solapur|\n|EMP101|Kolhapur|\n|EMP102|  Nagpur|\n+------+--------+\n\nroot\n |-- Emp_id: string (nullable = true)\n |-- city: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import get_json_object\n",
    "df1 = df.select('Emp_id', get_json_object('pdetails','$.address.city').alias('city'))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2081455385242071,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06_UDF_RDD",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}