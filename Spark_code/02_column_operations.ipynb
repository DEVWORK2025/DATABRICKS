{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b67103e-4ddd-44fd-812a-e186258bd25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'Nilesh','55000'),(2,'Nita','90000')]\n",
    "columns =['id','name','salary']\n",
    "df = spark.createDataFrame(data = data1, schema= columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92408305-3ecc-4a39-a639-001b40ad4786",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cb1f8f2-9cba-403d-8056-229ada91cc11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  name|salary|salary1|\n+---+------+------+-------+\n|  1|Nilesh| 55000|  55000|\n|  2|  Nita| 90000|  90000|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1 = df.withColumn(colName='salary1', col=col('salary').cast('Integer'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92211b75-974c-4527-b1c7-449689d7b882",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method withColumn in module pyspark.sql.dataframe:\n\nwithColumn(colName: str, col: pyspark.sql.column.Column) -> 'DataFrame' method of pyspark.sql.dataframe.DataFrame instance\n    Returns a new :class:`DataFrame` by adding a column or replacing the\n    existing column that has the same name.\n    \n    The column expression must be an expression over this :class:`DataFrame`; attempting to add\n    a column from some other :class:`DataFrame` will raise an error.\n    \n    .. versionadded:: 1.3.0\n    \n    .. versionchanged:: 3.4.0\n        Support Spark Connect.\n    \n    Parameters\n    ----------\n    colName : str\n        string, name of the new column.\n    col : :class:`Column`\n        a :class:`Column` expression for the new column.\n    \n    Returns\n    -------\n    :class:`DataFrame`\n        DataFrame with new or replaced column.\n    \n    Notes\n    -----\n    This method introduces a projection internally. Therefore, calling it multiple\n    times, for instance, via loops in order to add multiple columns can generate big\n    plans which can cause performance issues and even `StackOverflowException`.\n    To avoid this, use :func:`select` with multiple columns at once.\n    \n    Examples\n    --------\n    >>> df = spark.createDataFrame([(2, \"Alice\"), (5, \"Bob\")], schema=[\"age\", \"name\"])\n    >>> df.withColumn('age2', df.age + 2).show()\n    +---+-----+----+\n    |age| name|age2|\n    +---+-----+----+\n    |  2|Alice|   4|\n    |  5|  Bob|   7|\n    +---+-----+----+\n\n"
     ]
    }
   ],
   "source": [
    "help(df.withColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40ff9da7-e8e2-4fef-aa22-0e9552ea8037",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-756602108440970>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col\n",
       "\u001B[0;32m----> 2\u001B[0m df1 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(colName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary1\u001B[39m\u001B[38;5;124m'\u001B[39m, col\u001B[38;5;241m=\u001B[39mcol(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInteger\u001B[39m\u001B[38;5;124m'\u001B[39m))\\\n",
       "\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumns(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary2\u001B[39m\u001B[38;5;124m'\u001B[39m, col\u001B[38;5;241m=\u001B[39mcol(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m'\u001B[39m))\n",
       "\u001B[1;32m      4\u001B[0m df1\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: withColumns() got an unexpected keyword argument 'col'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-756602108440970>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfunctions\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m col\n\u001B[0;32m----> 2\u001B[0m df1 \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mwithColumn(colName\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary1\u001B[39m\u001B[38;5;124m'\u001B[39m, col\u001B[38;5;241m=\u001B[39mcol(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mInteger\u001B[39m\u001B[38;5;124m'\u001B[39m))\\\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;241m.\u001B[39mwithColumns(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary2\u001B[39m\u001B[38;5;124m'\u001B[39m, col\u001B[38;5;241m=\u001B[39mcol(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m)\u001B[38;5;241m.\u001B[39mcast(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfloat\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m      4\u001B[0m df1\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\n\u001B[0;31mTypeError\u001B[0m: withColumns() got an unexpected keyword argument 'col'",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: withColumns() got an unexpected keyword argument 'col'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df1 = df.withColumn(colName='salary1', col=col('salary').cast('Integer'))\\\n",
    "    .withColumns('salary2', col=col('salary').cast('float'))\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f8ec240-b438-45ec-b242-db3eeb627a46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: string (nullable = true)\n |-- salary1: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51e7e02-d26c-4776-aded-3a07d762c212",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+\n| id|  name|salary|salary1|\n+---+------+------+-------+\n|  1|Nilesh| 55000|  55000|\n|  2|  Nita| 90000|  90000|\n+---+------+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65eba595-a5f9-4107-885e-b5f3bbff978d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+----------+\n| id|  name|salary|salary1|commission|\n+---+------+------+-------+----------+\n|  1|Nilesh| 55000|  55000|    2750.0|\n|  2|  Nita| 90000|  90000|    4500.0|\n+---+------+------+-------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df1.withColumn('commission', col('salary')*0.05)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c57bd787-4ae5-4830-aa38-3aa27dc7dbb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = df1.withColumn('salary', col('salary')*0.05)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6529d7f7-af02-4110-b16b-2bf2e65eadce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+-------+----------+---------+\n| id|  name|salary|salary1|commission|  company|\n+---+------+------+-------+----------+---------+\n|  1|Nilesh| 55000|  55000|    2750.0|bizmetric|\n|  2|  Nita| 90000|  90000|    4500.0|bizmetric|\n+---+------+------+-------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "## passing hard coded values and creating new columns\n",
    "from pyspark.sql.functions import col, lit\n",
    "df3 = df2.withColumn('company', lit('bizmetric'))\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fa2d29c-a108-4d70-abae-66bb0181920c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+-------+----------+---------+\n| id|  name| sal1|salary1|commission|  company|\n+---+------+-----+-------+----------+---------+\n|  1|Nilesh|55000|  55000|    2750.0|bizmetric|\n|  2|  Nita|90000|  90000|    4500.0|bizmetric|\n+---+------+-----+-------+----------+---------+\n\n"
     ]
    }
   ],
   "source": [
    "## change the column name\n",
    "df4 = df3.withColumnRenamed('salary','sal1')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f843416a-e1d8-4f09-9280-e083f2bbe48f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-756602108440973>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m df4 \u001B[38;5;241m=\u001B[39m df3\u001B[38;5;241m.\u001B[39mwithColumnRenamed({(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msal1\u001B[39m\u001B[38;5;124m'\u001B[39m), (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfname\u001B[39m\u001B[38;5;124m'\u001B[39m)})\n",
       "\u001B[1;32m      2\u001B[0m df4\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: withColumnRenamed() missing 1 required positional argument: 'new'"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-756602108440973>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m df4 \u001B[38;5;241m=\u001B[39m df3\u001B[38;5;241m.\u001B[39mwithColumnRenamed({(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msalary\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msal1\u001B[39m\u001B[38;5;124m'\u001B[39m), (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mname\u001B[39m\u001B[38;5;124m'\u001B[39m,\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfname\u001B[39m\u001B[38;5;124m'\u001B[39m)})\n\u001B[1;32m      2\u001B[0m df4\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\n\u001B[0;31mTypeError\u001B[0m: withColumnRenamed() missing 1 required positional argument: 'new'",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: withColumnRenamed() missing 1 required positional argument: 'new'",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df4 = df3.withColumnRenamed({('salary','sal1'), ('name','fname')})\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a27a883-9b8c-453a-b6cf-9a60c5f36563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+--------------------------------+\n|id |name1 |salary|Address                         |\n+---+------+------+--------------------------------+\n|1  |Nilesh|55000 |{203, A2, A PARK, Senapati marg}|\n|2  |Nita  |90000 |{205, A3, B PARK, Tilak marg}   |\n+---+------+------+--------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data1 = [(1,'Nilesh',55000, (203, 'A2', 'A PARK', 'Senapati marg')),(2,'Nita',90000,(205, 'A3', 'B PARK', 'Tilak marg'))]\n",
    "###columns =['id','name1','salary', 'Address']\n",
    "\n",
    "address = StructType([StructField(name = 'flat_num', dataType = IntegerType()),\\\n",
    "                       StructField(name = 'build_num', dataType = StringType()),\\\n",
    "                           StructField(name = 'build_name', dataType = StringType()),\\\n",
    "                               StructField(name = 'road', dataType = StringType())])\n",
    "\n",
    "schema1 = StructType([StructField(name = 'id', dataType = IntegerType()),\\\n",
    "                       StructField(name = 'name1', dataType = StringType()),\\\n",
    "                           StructField(name = 'salary', dataType = IntegerType()),\\\n",
    "                               StructField(name = 'Address', dataType = address)])\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema= schema1)\n",
    "df.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2da0037-f048-401c-a4e5-82f493ecfd75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: integer (nullable = true)\n |-- name1: string (nullable = true)\n |-- salary: integer (nullable = true)\n |-- Address: struct (nullable = true)\n |    |-- flat_num: integer (nullable = true)\n |    |-- build_num: string (nullable = true)\n |    |-- build_name: string (nullable = true)\n |    |-- road: string (nullable = true)\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name1</th><th>salary</th><th>Address</th></tr></thead><tbody><tr><td>1</td><td>Nilesh</td><td>55000</td><td>List(203, A2, A PARK, Senapati marg)</td></tr><tr><td>2</td><td>Nita</td><td>90000</td><td>List(205, A3, B PARK, Tilak marg)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Nilesh",
         55000,
         [
          203,
          "A2",
          "A PARK",
          "Senapati marg"
         ]
        ],
        [
         2,
         "Nita",
         90000,
         [
          205,
          "A3",
          "B PARK",
          "Tilak marg"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "name1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "salary",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Address",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"flat_num\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}},{\"name\":\"build_num\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"build_name\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"road\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}}]}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.printSchema()\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a2b6e7-2739-491e-ac0e-784aca59b87c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9724b78-d801-483d-aefd-06e690638ec3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3da02c76-5b91-41b9-af32-811ff6489753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n| id| name1|   numbers|\n+---+------+----------+\n|  1|Nilesh|[203, 204]|\n|  2|  Nita|  [90, 80]|\n+---+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data1 = [(1,'Nilesh',[203,204 ]),(2,'Nita',[90,80])]\n",
    "columns =['id','name1', 'numbers']\n",
    "\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema= columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "029c8724-ca0e-4aca-bdf6-2ac9329a9091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name1: string (nullable = true)\n |-- numbers: array (nullable = true)\n |    |-- element: long (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2df5a0b-b687-4cec-b77e-04d8b65a8d6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+\n| id| name1|    number|\n+---+------+----------+\n|  1|Nilesh|[203, 204]|\n|  2|  Nita|  [90, 80]|\n+---+------+----------+\n\nroot\n |-- id: integer (nullable = true)\n |-- name1: string (nullable = true)\n |-- number: array (nullable = true)\n |    |-- element: integer (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "data1 = [(1,'Nilesh',[203,204] ),(2,'Nita',[90,80])]\n",
    "#columns =['id','name1', 'numbers']\n",
    "\n",
    "schema1 = StructType([StructField(name = 'id', dataType = IntegerType()),\\\n",
    "                       StructField(name = 'name1', dataType = StringType()),\\\n",
    "                           StructField(name = 'number', dataType =ArrayType(IntegerType()))])\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = schema1)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b327fc2c-f73a-4670-955f-0955cc3f86e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n|number[1]|\n+---------+\n|      204|\n|       80|\n+---------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(col('number')[1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30bf0f21-1336-4db0-9cfc-0f32c4176a94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------+-------+\n| id| name1|    number|new_col|\n+---+------+----------+-------+\n|  1|Nilesh|[203, 204]|    203|\n|  2|  Nita|  [90, 80]|     90|\n+---+------+----------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "df.withColumn('new_col', col('number')[0]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bb9d039b-2457-4097-bed6-c914df804ac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "schema1 = StructType([StructField(name = 'id', dataType = IntegerType()),\\\n",
    "                       StructField(name = 'name1', dataType = StringType()),\\\n",
    "                           StructField(name = 'number', dataType = ArrayType(IntegerType()))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9722c91-eed2-4dbd-ba70-1f52802b45cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n|name1|numbers|\n+-----+-------+\n|  203|    204|\n|   90|     80|\n+-----+-------+\n\nroot\n |-- name1: long (nullable = true)\n |-- numbers: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(203,204 ),(90,80)]\n",
    "columns =['name1', 'numbers']\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a40c247-2cba-40f0-96db-acbd63f735b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+----------+\n|name1|numbers|Collection|\n+-----+-------+----------+\n|  203|    204|[203, 204]|\n|   90|     80|  [90, 80]|\n+-----+-------+----------+\n\nroot\n |-- name1: long (nullable = true)\n |-- numbers: long (nullable = true)\n |-- Collection: array (nullable = false)\n |    |-- element: long (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, array\n",
    "data1 = [(203,204 ),(90,80)]\n",
    "columns =['name1', 'numbers']\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "df2 = df.withColumn('Collection', array(col('name1'),col('numbers')))\n",
    "df2.show()\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "972e1f6c-c56d-46ff-8912-b4a00c5ceed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Operations on array type columns\n",
    "explode()\n",
    "\n",
    "split()\n",
    "\n",
    "array()\n",
    "\n",
    "array_contains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98d5c4d4-8267-43e5-80a5-c4424b65bb5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>games</th></tr></thead><tbody><tr><td>2</td><td>Sunita</td><td>List(football, Archary)</td></tr><tr><td>3</td><td>Fatima</td><td>List(Baseball, chess)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "Sunita",
         [
          "football",
          "Archary"
         ]
        ],
        [
         3,
         "Fatima",
         [
          "Baseball",
          "chess"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "games",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":true}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- games: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, array\n",
    "data1 = [(2,'Sunita', ['football','Archary']),(3,'Fatima',['Baseball','chess'])]\n",
    "columns =['id', 'name', 'games']\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68ce1eca-3927-4c4d-852a-887ddd0747e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------------+\n| id|  name|              games|\n+---+------+-------------------+\n|  2|Sunita|[football, Archary]|\n|  3|Fatima|  [Baseball, chess]|\n+---+------+-------------------+\n\n+---+------+-------------------+------------+\n| id|  name|              games|game_details|\n+---+------+-------------------+------------+\n|  2|Sunita|[football, Archary]|    football|\n|  2|Sunita|[football, Archary]|     Archary|\n|  3|Fatima|  [Baseball, chess]|    Baseball|\n|  3|Fatima|  [Baseball, chess]|       chess|\n+---+------+-------------------+------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- games: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- game_details: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df.show()\n",
    "df_Exp = df.withColumn('game_details', explode(col('games')))\n",
    "df_Exp.show()\n",
    "df_Exp.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba5fd5e9-ee14-4ddd-9fd3-512585195b2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+----------------+\n| id|  name|           games|\n+---+------+----------------+\n|  2|Sunita|football,Archary|\n|  3|Fatima|  Baseball,chess|\n+---+------+----------------+\n\n+---+------+----------------+-------------------+\n| id|  name|           games|       game_details|\n+---+------+----------------+-------------------+\n|  2|Sunita|football,Archary|[football, Archary]|\n|  3|Fatima|  Baseball,chess|  [Baseball, chess]|\n+---+------+----------------+-------------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- games: string (nullable = true)\n |-- game_details: array (nullable = true)\n |    |-- element: string (containsNull = false)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "data1 = [(2,'Sunita', 'football,Archary'),(3,'Fatima','Baseball,chess')]\n",
    "columns =['id', 'name', 'games']\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "\n",
    "df.show()\n",
    "df_split = df.withColumn('game_details', split('games', ','))\n",
    "df_split.show()\n",
    "df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a9fe58b-88ac-421a-b67f-0727e7914837",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>id</th><th>name</th><th>games</th><th>game_details</th></tr></thead><tbody><tr><td>2</td><td>Sunita</td><td>football,Archary</td><td>List(football, Archary)</td></tr><tr><td>3</td><td>Fatima</td><td>Baseball,chess</td><td>List(Baseball, chess)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         2,
         "Sunita",
         "football,Archary",
         [
          "football",
          "Archary"
         ]
        ],
        [
         3,
         "Fatima",
         "Baseball,chess",
         [
          "Baseball",
          "chess"
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "id",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "games",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "game_details",
         "type": "{\"type\":\"array\",\"elementType\":\"string\",\"containsNull\":false}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c7271c0-3701-4a03-9b7e-81fa7862915d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data1 = [(2,'Sunita', 'football','Archary'),(3,'Fatima','Baseball','chess')]\n",
    "columns =['id', 'name', 'game1','game2']\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "\n",
    "df.show()\n",
    "#df_split = df.withColumn('game_details', split('games', ','))\n",
    "#df_split.show()\n",
    "#df_split.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1b13519-5c68-4dc7-8a08-7cfb7e9693da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import array,col\n",
    "\n",
    "df2 = df.withColumn('GAMES', array(col('game1'), col('game2')))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbdefd8e-a065-471e-85f8-c51d0c47a4fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------------+\n| id|  name|              games|\n+---+------+-------------------+\n|  2|Sunita|[football, Archary]|\n|  3|Fatima|  [Baseball, chess]|\n+---+------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(2,'Sunita', ['football','Archary']),(3,'Fatima',['Baseball', 'chess'])]\n",
    "columns =['id', 'name', 'games']\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "584c264a-7058-4a64-813b-cebd9afabcf5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------------------+-----------+\n| id|  name|              games|check_chess|\n+---+------+-------------------+-----------+\n|  2|Sunita|[football, Archary]|      false|\n|  3|Fatima|  [Baseball, chess]|       true|\n+---+------+-------------------+-----------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- games: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- check_chess: boolean (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "### check if particular element is avaialble or not\n",
    "from pyspark.sql.functions import array_contains, col\n",
    "\n",
    "\n",
    "df_check = df.withColumn('check_chess', array_contains(col('games'), 'chess'))\n",
    "df_check.show()\n",
    "df_check.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c98c4d4a-13fc-4bbe-b78a-ed70162e8a7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### MAP TYPE column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa5eca39-7510-4674-a752-12d9f8a58097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------------------+\n|name  |games                          |\n+------+-------------------------------+\n|Sunita|{g1 -> football, g2 -> Archary}|\n|Fatima|{g1 -> Baseball, g2 -> chess}  |\n+------+-------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, MapType\n",
    "\n",
    "data1 = [('Sunita', {'g1': 'football', 'g2': 'Archary'}),('Fatima',{'g1': 'Baseball', 'g2': 'chess'})]\n",
    "columns =['name', 'games']\n",
    "\n",
    "#columns = StructType([ StructField('name', StringType()),  StructField('games', MapType(StringType(), StringType()))  ])\n",
    "\n",
    "df = spark.createDataFrame(data = data1, schema = columns)\n",
    "\n",
    "df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b2747e-4818-4096-bc2c-121cf553cc0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- games: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c498670-d86a-4164-a636-f4bd35b0dbdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### accessing elements from map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e953d3c-2219-4f26-8208-19735c1c2d29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------+\n|  name|               games|   game1|\n+------+--------------------+--------+\n|Sunita|{g1 -> football, ...|football|\n|Fatima|{g1 -> Baseball, ...|Baseball|\n+------+--------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df2 = df.withColumn('game1', df.games['g1'])\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91c5646e-caeb-4a44-aa64-7f9e1a7ce057",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df2 = df.withColumn('game1', df.games.getItem('g1'))\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68e0f437-5833-438e-8486-9ed074d90158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "explode()\n",
    "\n",
    "map_keys()\n",
    "\n",
    "map_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07918197-d32d-42cc-9338-de8a2f710120",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n|  name|               games|\n+------+--------------------+\n|Sunita|{g1 -> football, ...|\n|Fatima|{g1 -> Baseball, ...|\n+------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7cec27dd-236d-4942-9075-1f2132117bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+---+--------+\n|games                          |key|value   |\n+-------------------------------+---+--------+\n|{g1 -> football, g2 -> Archary}|g1 |football|\n|{g1 -> football, g2 -> Archary}|g2 |Archary |\n|{g1 -> Baseball, g2 -> chess}  |g1 |Baseball|\n|{g1 -> Baseball, g2 -> chess}  |g2 |chess   |\n+-------------------------------+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df_map_exp = df.select('name','games',explode(df.games))\n",
    "df_map_exp.show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcabaab6-9ffb-48c3-b58e-66e056e1290e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------+\n|  name|               games|    keys|\n+------+--------------------+--------+\n|Sunita|{g1 -> football, ...|[g1, g2]|\n|Fatima|{g1 -> Baseball, ...|[g1, g2]|\n+------+--------------------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "\n",
    "df_map_keys = df.withColumn('keys', map_keys(df.games))\n",
    "df_map_keys.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a7444b-3c55-4ab4-9cf0-ad27a2de3db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- name: string (nullable = true)\n |-- games: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- keys: array (nullable = true)\n |    |-- element: string (containsNull = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_map_keys.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d3dadd2-3550-4e75-8b24-dbc5f7b211ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+-------------------+\n|  name|               games|             values|\n+------+--------------------+-------------------+\n|Sunita|{g1 -> football, ...|[football, Archary]|\n|Fatima|{g1 -> Baseball, ...|  [Baseball, chess]|\n+------+--------------------+-------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "\n",
    "df_map_values = df.withColumn('values', map_values(df.games))\n",
    "df_map_values.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66ae703f-ccbd-4182-b000-cc0fcf2f6e2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ROW\n",
    "\n",
    "#### from pyspark.sql import row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c2b0a4-7726-4d6d-9de2-87b199028a2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Row(1, 'Nishad')>\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "r1 = Row(1,'Nishad')\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2154b356-1380-43cc-8df8-f162a1fdcafb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: pyspark.sql.types.Row"
     ]
    }
   ],
   "source": [
    "type(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbb7c44-cd3d-46fb-8864-d6dd5bdaba46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Nishad\n1 Nishad\n"
     ]
    }
   ],
   "source": [
    "print(r1[0], r1[1])\n",
    "print(str(r1[0]), r1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa4f1a74-8fef-4fbd-8c42-69ca4bed3ce1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "r1 = Row(id= 1, name= 'Nishad')\n",
    "print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a761531-4389-4bef-a0ed-fa5b1eae68e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "r1['id'],  r1['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d99c7c4-ead5-4862-92d1-065319c5893f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n| id|  name|\n+---+------+\n|  1|Nishad|\n|  2| Nisha|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "r1 = Row(id= 1, name= 'Nishad')\n",
    "r2= Row(id= 2, name= 'Nisha')\n",
    "data =[r1, r2]\n",
    "\n",
    "df= spark.createDataFrame(data)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25609707-5c4d-40b7-9ed1-1cd2c8f58aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Navin\n+---+------+\n| id|  name|\n+---+------+\n|  1| Navin|\n|  2|Nalini|\n|  3|Nilima|\n+---+------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "### creating class of row with customized values\n",
    "student = Row('id','name')\n",
    "\n",
    "r1= student(1,'Navin')\n",
    "r2= student(2,'Nalini')\n",
    "r3= student(3,'Nilima')\n",
    "\n",
    "print(r1.id, r1.name)\n",
    "\n",
    "data1 =[r1, r2,r3 ]\n",
    "\n",
    "df= spark.createDataFrame(data1)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e4b716-5a5e-4129-bb67-d8a3806574fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Navin\n+---+------+------------+\n| id|  name|        rank|\n+---+------+------------+\n|  1| Navin|{sketing, 3}|\n|  2|Nalini|{sketing, 3}|\n|  3|Nilima|{sketing, 3}|\n+---+------+------------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- rank: struct (nullable = true)\n |    |-- g1: string (nullable = true)\n |    |-- rank: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "### creating class of row with customized values\n",
    "rank = Row('g1','rank')\n",
    "student = Row('id','name', 'rank')\n",
    "\n",
    "r1= student(1,'Navin', rank('sketing','3'))\n",
    "r2= student(2,'Nalini', rank('sketing','3'))\n",
    "r3= student(3,'Nilima', rank('sketing','3'))\n",
    "\n",
    "print(r1.id, r1.name)\n",
    "\n",
    "data1 =[r1, r2,r3 ]\n",
    "\n",
    "df= spark.createDataFrame(data1)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f81e8b70-665f-4906-b2cc-ae2a3f0a212a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+\n| name|          game|\n+-----+--------------+\n|Nisha|    {chess, 2}|\n| Nima|    {chess, 3}|\n| Nila|{football, 10}|\n+-----+--------------+\n\nroot\n |-- name: string (nullable = true)\n |-- game: struct (nullable = true)\n |    |-- g1: string (nullable = true)\n |    |-- rank: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [Row(name ='Nisha', game = Row(g1='chess', rank =2)),\\\n",
    "        Row(name ='Nima', game = Row(g1 ='chess', rank =3)),\\\n",
    "             Row(name ='Nila', game = Row(g1 ='football', rank =10))]\n",
    "\n",
    "df =  spark.createDataFrame(data)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c7555f-e21a-49f8-8134-bd081db6e575",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>name</th><th>game</th></tr></thead><tbody><tr><td>Nisha</td><td>List(chess, 2)</td></tr><tr><td>Nima</td><td>List(chess, 3)</td></tr><tr><td>Nila</td><td>List(football, 10)</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Nisha",
         [
          "chess",
          2
         ]
        ],
        [
         "Nima",
         [
          "chess",
          3
         ]
        ],
        [
         "Nila",
         [
          "football",
          10
         ]
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "game",
         "type": "{\"type\":\"struct\",\"fields\":[{\"name\":\"g1\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"rank\",\"type\":\"long\",\"nullable\":true,\"metadata\":{}}]}"
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a0591659-4d82-4eab-9a2c-a8d480f05be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## COL operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf8d791-66ba-4c5f-8d81-5410f8fdce2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: pyspark.sql.column.Column"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "\n",
    "col1= lit('abcd')\n",
    "type(col1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39491f07-303f-4e56-9d80-13f0774b8739",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----+\n| name|    game|rank|\n+-----+--------+----+\n|Nisha|   chess|   2|\n| Nima|   chess|   3|\n| Nila|football|  10|\n+-----+--------+----+\n\nroot\n |-- name: string (nullable = true)\n |-- game: string (nullable = true)\n |-- rank: long (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [('Nisha','chess', 2),\\\n",
    "        ('Nima', 'chess', 3),\\\n",
    "             ('Nila', 'football',10)]\n",
    "\n",
    "schema1=['name', 'game', 'rank']\n",
    "\n",
    "df =  spark.createDataFrame(data, schema1)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed286bb-9f88-4262-b538-83b92c896693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----+---------+\n| name|    game|rank|   SCHOOL|\n+-----+--------+----+---------+\n|Nisha|   chess|   2|SYMBIOSIS|\n| Nima|   chess|   3|SYMBIOSIS|\n| Nila|football|  10|SYMBIOSIS|\n+-----+--------+----+---------+\n\nroot\n |-- name: string (nullable = true)\n |-- game: string (nullable = true)\n |-- rank: long (nullable = true)\n |-- SCHOOL: string (nullable = false)\n\n"
     ]
    }
   ],
   "source": [
    "df1 = df.withColumn('SCHOOL', lit(\"SYMBIOSIS\"))\n",
    "df1.show()\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2559108-d500-4408-879e-b37af34e185d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n| name|\n+-----+\n|Nisha|\n| Nima|\n| Nila|\n+-----+\n\n+-----+\n| name|\n+-----+\n|Nisha|\n| Nima|\n| Nila|\n+-----+\n\n+-----+--------+---------+\n| name|    game|   SCHOOL|\n+-----+--------+---------+\n|Nisha|   chess|SYMBIOSIS|\n| Nima|   chess|SYMBIOSIS|\n| Nila|football|SYMBIOSIS|\n+-----+--------+---------+\n\n+-----+--------+----+---------+\n| name|    game|rank|   SCHOOL|\n+-----+--------+----+---------+\n|Nisha|   chess|   2|SYMBIOSIS|\n| Nima|   chess|   3|SYMBIOSIS|\n| Nila|football|  10|SYMBIOSIS|\n+-----+--------+----+---------+\n\n+-----+--------+----+---------+\n| name|    game|rank|   SCHOOL|\n+-----+--------+----+---------+\n|Nisha|   chess|   2|SYMBIOSIS|\n| Nima|   chess|   3|SYMBIOSIS|\n| Nila|football|  10|SYMBIOSIS|\n+-----+--------+----+---------+\n\n+-----+\n| name|\n+-----+\n|Nisha|\n| Nima|\n| Nila|\n+-----+\n\n"
     ]
    }
   ],
   "source": [
    "### fecting the columns\n",
    "from pyspark.sql.functions import col\n",
    "df1.select(df1.name).show()\n",
    "df1.select(df1['name']).show()\n",
    "df1.select(['name','game','SCHOOL']).show()\n",
    "df1.select('*').show()\n",
    "df1.select([col for col in df1.columns]).show()\n",
    "df1.select(col('name')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8510e391-cfaa-4e12-95e9-53732c2f4527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+----+----------------+\n| name|    game|rank|     Compitition|\n+-----+--------+----+----------------+\n|Nisha|   chess|   2|{round1, satara}|\n| Nima|   chess|   3|  {round2, Pune}|\n| Nila|football|  10|  {final, Delhi}|\n+-----+--------+----+----------------+\n\nroot\n |-- name: string (nullable = true)\n |-- game: string (nullable = true)\n |-- rank: integer (nullable = true)\n |-- Compitition: struct (nullable = true)\n |    |-- round: string (nullable = true)\n |    |-- location: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "data = [('Nisha','chess', 2, ('round1','satara')),\\\n",
    "        ('Nima', 'chess', 3, ('round2','Pune')),\\\n",
    "             ('Nila', 'football',10, (\"final\",'Delhi'))]\n",
    "\n",
    "prop = StructType([ StructField( 'round',StringType() ),\\\n",
    "    StructField( 'location',StringType() )])\n",
    "\n",
    "schema1=StructType([StructField('name', StringType() ) ,\\\n",
    "            StructField( 'game',StringType() ),\\\n",
    "            StructField('rank', IntegerType()),\\\n",
    "            StructField('Compitition', prop )])\n",
    "\n",
    "df =  spark.createDataFrame(data, schema1)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e949eb67-01ae-444b-b281-9bbb1b05ada3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n|     Compitition|\n+----------------+\n|{round1, satara}|\n|  {round2, Pune}|\n|  {final, Delhi}|\n+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Compitition).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcffa686-283a-4e21-8a9c-8dbda02117f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|Compitition.location|\n+--------------------+\n|              satara|\n|                Pune|\n|               Delhi|\n+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df.select(df.Compitition.location).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e03fa74a-f2f7-4589-8e1a-92a5055ebec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n|location|\n+--------+\n|  satara|\n|    Pune|\n|   Delhi|\n+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.select(col('Compitition.location')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "323073c7-158d-4896-938e-7974cdda0e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## When and otherwise function in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1421987-9d06-4497-9569-3025ec03bae7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n| name|    game|location|\n+-----+--------+--------+\n|Nisha|   chess|    Pune|\n| Nima|   chess|  satara|\n| Nila|football|        |\n+-----+--------+--------+\n\nroot\n |-- name: string (nullable = true)\n |-- game: string (nullable = true)\n |-- location: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "data = [('Nisha','chess', 'Pune'),\\\n",
    "        ('Nima', 'chess', 'satara' ),\\\n",
    "             ('Nila', 'football','')]\n",
    "\n",
    "schema1 = ['name','game','location']\n",
    "\n",
    "df =  spark.createDataFrame(data, schema1)\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d33c816d-b017-475c-b46b-44b1dd7f2242",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n| name|    game|distance|\n+-----+--------+--------+\n|Nisha|   chess|  nearby|\n| Nima|   chess|     far|\n| Nila|football| UNKNOWN|\n+-----+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "df1 = df.select(df.name, df.game, when(df.location == \"Pune\" , 'nearby').when(df.location == \"satara\" , 'far').otherwise('UNKNOWN').alias('distance')   )\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de3313ab-b768-46be-a35a-b0c14f3c96c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## alias(), cast(), asc(), desc(), cast(), like() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fe6011a-87f1-4339-a9bc-8b7526bca8f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n+---+------+------+\n\n+------+----------+\n|emp_id|First_name|\n+------+----------+\n|     1|    Nilesh|\n|     2|      Nita|\n+------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'Nilesh','55000'),(2,'Nita','90000')]\n",
    "columns =['id','name','salary']\n",
    "df = spark.createDataFrame(data = data1, schema= columns)\n",
    "df.show()\n",
    "df.select(df.id.alias('emp_id'), df.name.alias(\"First_name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0190f5d1-16d3-4add-8d2f-c84996d449e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n+---+------+------+\n\n+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  2|  Nita| 90000|\n|  1|Nilesh| 55000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "### sorting the data\n",
    "data1 = [(1,'Nilesh','55000'),(2,'Nita','90000')]\n",
    "columns =['id','name','salary']\n",
    "df = spark.createDataFrame(data = data1, schema= columns)\n",
    "df.show()\n",
    "df.sort(df.name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d788a01-e058-43e2-ba45-a0e8898cbb2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n+---+------+-------+\n\nroot\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: string (nullable = true)\n\n+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  3|Sunita|  10000|\n|  2|  Nita|  90000|\n|  1|Nilesh|  55000|\n|  1|Nilesh|  55000|\n|  5|Nandan|1200000|\n|  4|Nandan| 100000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "### sorting the data\n",
    "data1 = [(1,'Nilesh','55000'),(2,'Nita','90000'),(3,'Sunita','10000'),(4,'Nandan','100000'),(5,'Nandan','1200000'),(1,'Nilesh','55000')]\n",
    "columns =['id','name','salary']\n",
    "df = spark.createDataFrame(data = data1, schema= columns)\n",
    "df.show()\n",
    "df.printSchema()\n",
    "df.sort(df.name.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a803e2a-81f6-4ed9-b6fb-99b35dde19f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- id: long (nullable = true)\n |-- name: string (nullable = true)\n |-- salary: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "## converting data type to integer\n",
    "df1 = df.select(df.id, df.name, df.salary.cast('int'))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8da1c1a4-cd5e-4c52-81ca-f887e46c51f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filter and where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "936cea55-d344-4618-8715-f65738a3a7c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  2|  Nita| 90000|\n|  3|Sunita| 10000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.name.like('%t%')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f7e9db-81ee-4350-b329-7f9dd5b135db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n|  4|Nandan|100000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter( df.salary >50000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdafc830-c611-4182-8136-d3919a664001",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n|  4|Nandan|100000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.filter( df.salary >50000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937c08c3-7bcc-4f73-9834-1dca6553d428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  4|Nandan|100000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df.where( (df.salary >50000) & ~(df.name.like ('%t%'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21710931-26ed-4113-8e90-2b13e2d8f0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "984df6a0-1195-4ca8-840c-2b43359f2def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdcca6af-e0aa-4a30-9b6e-6fd16d70958b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae51bf74-317f-45bf-88c3-38a63c938596",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: DataFrame[name: string, game: string, location: string]"
     ]
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4afd9c5a-5ad6-4495-a317-8e5da36c0281",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['name','salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "720bad0e-fd3e-4282-93e7-9dcb5b76cdd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce8a971f-2d14-4132-a7f1-b1dda2fe4015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Order by , sort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f601aa02-1dd9-46cf-a230-d8bad32d95d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49211f2a-c0d2-4d50-ab2a-b59257131ce3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort('name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5283efa7-91f4-47c9-a6f8-6b65bfb7ead1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b05f690-45b7-4e5d-83dd-31d37ab557e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  5|Nandan|1200000|\n|  4|Nandan| 100000|\n|  1|Nilesh|  55000|\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.sort(df.name, df.salary.desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3114498-5ffa-479b-a06d-cfbd49e5569a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(['name', 'salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fbd76b2-972c-437a-a945-0740e2647d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e5253a2-7b81-407a-ab41-7a5997922166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b8d0e5d-da8c-4244-b202-82fd28ebe828",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### sorting the data\n",
    "data1 = [(1,'Nilesh','55000'),(2,'Nita','90000'),(3,'Sunita','10000'),(4,'Nandan','100000'),(5,'Nandan','1200000'),(1,'Nilesh','55000')]\n",
    "columns =['id','name','salary']\n",
    "df1 = spark.createDataFrame(data = data1, schema= columns)\n",
    "\n",
    "df2 = spark.createDataFrame(data = data1, schema= columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81eeda4b-6048-4daa-9367-0153fbe1f379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e99291e-095d-48ac-908a-3768ebef0bdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96315bd5-131a-4b9f-9ddd-6f961b0da62e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+\n| name|    game|distance|\n+-----+--------+--------+\n|Nisha|   chess|  nearby|\n| Nima|   chess|     far|\n| Nila|football| UNKNOWN|\n|Nisha|   chess|  nearby|\n| Nima|   chess|     far|\n| Nila|football| UNKNOWN|\n+-----+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "new_df = df1.union(df1)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0858fd1-915d-4652-bb18-021c2005eff7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "new_df = df1.union(df2)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e54624bd-4ade-4aad-9eea-98148f4f6971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n| id|  name| salary|\n+---+------+-------+\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n|  1|Nilesh|  55000|\n|  2|  Nita|  90000|\n|  3|Sunita|  10000|\n|  4|Nandan| 100000|\n|  5|Nandan|1200000|\n|  1|Nilesh|  55000|\n+---+------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "new_df = df1.unionAll(df2)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4ad256f-760f-47b4-bd43-47f950a05a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n|  3|Sunita| 10000|\n|  5|Suresh| 50000|\n+---+------+------+\n\n+---+------+--------+\n| id|  name|location|\n+---+------+--------+\n|  4|Nandan|  SATARA|\n|  5|Nandan|    PUNE|\n|  6|Nilesh|  MUMBAI|\n+---+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'Nilesh',55000),(2,'Nita',90000),(3,'Sunita',10000),(5,'Suresh',50000)]\n",
    "columns1 =['id','name','salary']\n",
    "\n",
    "data2 = [(4,'Nandan','SATARA'),(5,'Nandan','PUNE'),(6,'Nilesh','MUMBAI')]\n",
    "columns2 =['id','name','location']\n",
    "\n",
    "\n",
    "df1 = spark.createDataFrame(data = data1, schema= columns1)\n",
    "df1.show()\n",
    "df2 = spark.createDataFrame(data = data2, schema= columns2)\n",
    "df2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "217e43cf-811e-4f43-9064-a7c2c72dfe99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n|  3|Sunita| 10000|\n|  5|Suresh| 50000|\n|  4|Nandan|SATARA|\n|  5|Nandan|  PUNE|\n|  6|Nilesh|MUMBAI|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.union(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c033ee6a-f49b-4ffc-8e64-943990acaf11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2053494685448580>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m### generate the error as column names are not same \u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n",
       "\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n",
       "\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m   3685\u001B[0m \n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n",
       "\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Cannot resolve column name \"salary\" among (id, name, location)."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2053494685448580>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m### generate the error as column names are not same \u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf2\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3738\u001B[0m, in \u001B[0;36mDataFrame.unionByName\u001B[0;34m(self, other, allowMissingColumns)\u001B[0m\n\u001B[1;32m   3682\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21munionByName\u001B[39m(\u001B[38;5;28mself\u001B[39m, other: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m, allowMissingColumns: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   3683\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns a new :class:`DataFrame` containing union of rows in this and another\u001B[39;00m\n\u001B[1;32m   3684\u001B[0m \u001B[38;5;124;03m    :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m   3685\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3736\u001B[0m \u001B[38;5;124;03m    +----+----+----+----+\u001B[39;00m\n\u001B[1;32m   3737\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 3738\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munionByName\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mallowMissingColumns\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Cannot resolve column name \"salary\" among (id, name, location).",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Cannot resolve column name \"salary\" among (id, name, location).",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### generate the error as column names are not same \n",
    "df1.unionByName(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33dc061d-054b-4a03-95bf-6f1091a46d4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+--------+\n| id|  name|salary|location|\n+---+------+------+--------+\n|  1|Nilesh| 55000|    null|\n|  2|  Nita| 90000|    null|\n|  3|Sunita| 10000|    null|\n|  5|Suresh| 50000|    null|\n|  4|Nandan|  null|  SATARA|\n|  5|Nandan|  null|    PUNE|\n|  6|Nilesh|  null|  MUMBAI|\n+---+------+------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.unionByName(allowMissingColumns=True, other = df2).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "150406a1-9eb6-41e4-8569-084cec7274fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## JOINS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f12b87-e7da-4b8a-960a-91fc9caec75b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n|  3|Sunita| 10000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8661e2f-bae2-431b-9515-f75843ddd22b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n|  3|Sunita| 10000|\n|  5|Suresh| 50000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8352f36-51c2-406e-80c7-186c199fb80e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|location|\n+---+--------+\n|  1|   Dhule|\n|  2|   Latur|\n|  3|  JALGAO|\n|  4|    BEED|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "data3 = [(1,'Dhule'),(2,'Latur'),(3,'JALGAO'), (4,'BEED')]\n",
    "columns3 = ['id','location']\\\n",
    "\n",
    "df3 = spark.createDataFrame(data3, columns3)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa4c6f65-ae66-4aa6-8b33-fee5502b187b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+---+--------+\n| id|  name|salary| id|location|\n+---+------+------+---+--------+\n|  1|Nilesh| 55000|  1|   Dhule|\n|  2|  Nita| 90000|  2|   Latur|\n|  3|Sunita| 10000|  3|  JALGAO|\n+---+------+------+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "new_df =df1.join(df3, df1.id == df3.id, 'inner')\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ce9c3a-3736-40e1-9eb3-4cb1a680c9fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+----+--------+\n| id|  name|salary|  id|location|\n+---+------+------+----+--------+\n|  1|Nilesh| 55000|   1|   Dhule|\n|  2|  Nita| 90000|   2|   Latur|\n|  3|Sunita| 10000|   3|  JALGAO|\n|  5|Suresh| 50000|null|    null|\n+---+------+------+----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.join(df3, df1.id == df3.id, 'left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb1d6cf-ec6a-4f35-9628-a5f926821735",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+---+--------+\n|  id|  name|salary| id|location|\n+----+------+------+---+--------+\n|   1|Nilesh| 55000|  1|   Dhule|\n|   2|  Nita| 90000|  2|   Latur|\n|   3|Sunita| 10000|  3|  JALGAO|\n|null|  null|  null|  4|    BEED|\n+----+------+------+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.join(df3, df1.id == df3.id, 'right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d48855-5b9d-4dd3-b1e1-f6f66b79b027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+------+----+--------+\n|  id|  name|salary|  id|location|\n+----+------+------+----+--------+\n|   1|Nilesh| 55000|   1|   Dhule|\n|   2|  Nita| 90000|   2|   Latur|\n|   3|Sunita| 10000|   3|  JALGAO|\n|null|  null|  null|   4|    BEED|\n|   5|Suresh| 50000|null|    null|\n+----+------+------+----+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.join(df3, df1.id == df3.id, 'full').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0949430f-c3ea-4689-ac4a-06c4c5281fd0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### SELF, LEFT SEMI, LEFT ANTI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8257d890-bdae-4be1-9aa2-c3a8f56c49d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n|  3|Sunita| 10000|\n|  5|Suresh| 50000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66b44b5f-11e0-4593-8b50-4fa18ca1cfaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n| id|location|\n+---+--------+\n|  1|   Dhule|\n|  2|   Latur|\n|  3|  JALGAO|\n|  4|    BEED|\n+---+--------+\n\n"
     ]
    }
   ],
   "source": [
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1d79f0d-c2bd-4749-9aaf-a91f038a46b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  1|Nilesh| 55000|\n|  2|  Nita| 90000|\n|  3|Sunita| 10000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "## return only left data columns\n",
    "df1.join(df3, df1.id == df3.id, 'leftsemi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e2e8fb-9aab-4d0c-a605-ecd54950c030",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-142351326289122>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdf3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrightsemi\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2364\u001B[0m, in \u001B[0;36mDataFrame.join\u001B[0;34m(self, other, on, how)\u001B[0m\n",
       "\u001B[1;32m   2362\u001B[0m         on \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq([])\n",
       "\u001B[1;32m   2363\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(how, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m-> 2364\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: Unsupported join type 'rightsemi'. Supported join types include: 'inner', 'outer', 'full', 'fullouter', 'full_outer', 'leftouter', 'left', 'left_outer', 'rightouter', 'right', 'right_outer', 'leftsemi', 'left_semi', 'semi', 'leftanti', 'left_anti', 'anti', 'cross'."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-142351326289122>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf3\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdf1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdf3\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrightsemi\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshow()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:2364\u001B[0m, in \u001B[0;36mDataFrame.join\u001B[0;34m(self, other, on, how)\u001B[0m\n\u001B[1;32m   2362\u001B[0m         on \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jseq([])\n\u001B[1;32m   2363\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(how, \u001B[38;5;28mstr\u001B[39m), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhow should be a string\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 2364\u001B[0m     jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mother\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mon\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhow\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2365\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: Unsupported join type 'rightsemi'. Supported join types include: 'inner', 'outer', 'full', 'fullouter', 'full_outer', 'leftouter', 'left', 'left_outer', 'rightouter', 'right', 'right_outer', 'leftsemi', 'left_semi', 'semi', 'leftanti', 'left_anti', 'anti', 'cross'.",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: Unsupported join type 'rightsemi'. Supported join types include: 'inner', 'outer', 'full', 'fullouter', 'full_outer', 'leftouter', 'left', 'left_outer', 'rightouter', 'right', 'right_outer', 'leftsemi', 'left_semi', 'semi', 'leftanti', 'left_anti', 'anti', 'cross'.",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df1.join(df3, df1.id == df3.id, 'rightsemi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8c2f887-6017-414a-8cc6-aaf7c7e37fed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n| id|  name|salary|\n+---+------+------+\n|  5|Suresh| 50000|\n+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "## return only right data columns\n",
    "df1.join(df3, df1.id == df3.id, 'leftanti').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec607e55-7b6d-4cf5-81b9-3204ff44b8f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "## return only left data columns\n",
    "df1.join(df3, df1.id == df3.id, 'leftsemi').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebcfda3f-90f4-42a4-a8c2-a722176c4802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+\n| id|   name|mgr_id|\n+---+-------+------+\n|  1| naresh|  null|\n|  2| Ramesh|     1|\n|  3| Suresh|     2|\n|  4|Jignesh|     2|\n+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "data1 = [(1,'naresh',None),(2,'Ramesh',1),(3,'Suresh',2),(4,'Jignesh',2)]\n",
    "columns4 = ['id','name','mgr_id']\n",
    "\n",
    "df4 = spark.createDataFrame(data1, columns4)\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41beacdb-9a01-436e-b9ed-873a39cbefe1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df4.join(d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3338b3d4-6457-4ff3-9c87-62f27b5ee448",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+---+------+------+\n| id|   name|mgr_id| id|  name|mgr_id|\n+---+-------+------+---+------+------+\n|  2| Ramesh|     1|  1|naresh|  null|\n|  3| Suresh|     2|  2|Ramesh|     1|\n|  4|Jignesh|     2|  2|Ramesh|     1|\n+---+-------+------+---+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "## display manager name against name\n",
    "from pyspark.sql.functions import col\n",
    "##df1.join(df3, df1.id == df3.id, 'self').show()\n",
    "df4.alias('emp').join(df4.alias('mgr') , col('emp.mgr_id') == col('mgr.id'), 'inner').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e11f798d-09e1-47dc-948f-d7bd59c2e189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------+----+------+------+\n| id|   name|mgr_id|  id|  name|mgr_id|\n+---+-------+------+----+------+------+\n|  1| naresh|  null|null|  null|  null|\n|  2| Ramesh|     1|   1|naresh|  null|\n|  3| Suresh|     2|   2|Ramesh|     1|\n|  4|Jignesh|     2|   2|Ramesh|     1|\n+---+-------+------+----+------+------+\n\n"
     ]
    }
   ],
   "source": [
    "## display manager name against name\n",
    "from pyspark.sql.functions import col\n",
    "##df1.join(df3, df1.id == df3.id, 'self').show()\n",
    "df4.alias('emp').join(df4.alias('mgr') , col('emp.mgr_id') == col('mgr.id'), 'left').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d550a948-c9ac-4c9a-b6ac-47dfed808584",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n|EMP_NAME|MGR_NAME|\n+--------+--------+\n|  naresh|    null|\n|  Ramesh|  naresh|\n|  Suresh|  Ramesh|\n| Jignesh|  Ramesh|\n+--------+--------+\n\n"
     ]
    }
   ],
   "source": [
    "## display manager name against name\n",
    "from pyspark.sql.functions import col\n",
    "##df1.join(df3, df1.id == df3.id, 'self').show()\n",
    "df4.alias('emp').join(df4.alias('mgr') , col('emp.mgr_id') == col('mgr.id'), 'left')\\\n",
    "    .select(col('emp.name').alias('EMP_NAME'),  col('mgr.name').alias('MGR_NAME')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26067c3a-9c40-4d19-b31b-e7be2294aaec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n| C1| C2|\n+---+---+\n|  c|  4|\n+---+---+\n\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])\n",
    "df1.subtract(df2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a400bf2-5f4d-42ef-8547-206b541db601",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3270515310027760>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m row1\u001B[38;5;241m=\u001B[39m \u001B[43mdf4\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n",
       "\n",
       "\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)\nFile \u001B[0;32m<command-3270515310027760>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m row1\u001B[38;5;241m=\u001B[39m \u001B[43mdf4\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m]\u001B[49m\n\n\u001B[0;31mTypeError\u001B[0m: list indices must be integers or slices, not tuple",
       "errorSummary": "<span class='ansi-red-fg'>TypeError</span>: list indices must be integers or slices, not tuple",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "row1= df4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bcac65-cd48-4365-8786-6995e1ecdda9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[46]: (1, None)"
     ]
    }
   ],
   "source": [
    "row1[0][0:3:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c184535-f9d4-4846-a1f7-851f61d53d8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, None)\n(2, 1)\n(3, 2)\n(4, 2)\n"
     ]
    }
   ],
   "source": [
    "cnt=0\n",
    "for i in row1:\n",
    "    print(row1[cnt][0:3:2])\n",
    "    cnt=cnt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3e7497d-ebbd-4735-8581-883d30db98a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[57]: [(1, None), (2, 1), (3, 2), (4, 2)]"
     ]
    }
   ],
   "source": [
    "[ i[0:3:2] for i in row1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb154cfa-f0f3-4e6c-bf04-726d06b3bc1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "var = row1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146f8e19-3086-4408-80ab-97820d9c0e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;36m  File \u001B[0;32m<command-1294697084394451>:1\u001B[0;36m\u001B[0m\n",
       "\u001B[0;31m    var[i for i in ['id','mgr_id']]\u001B[0m\n",
       "\u001B[0m          ^\u001B[0m\n",
       "\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;36m  File \u001B[0;32m<command-1294697084394451>:1\u001B[0;36m\u001B[0m\n\u001B[0;31m    var[i for i in ['id','mgr_id']]\u001B[0m\n\u001B[0m          ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid syntax\n",
       "errorSummary": "<span class='ansi-red-fg'>SyntaxError</span>: invalid syntax (<command-1294697084394451>, line 1)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "var[i for i in ['id','mgr_id']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dbcd030-7233-4643-b75b-41b38137eee2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: Row(id=1, name='naresh', mgr_id=None)"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4cb57d1-cb56-46c8-8730-fd93bd1586c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[13]: 'naresh'"
     ]
    }
   ],
   "source": [
    "row1[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf0bf20f-0cf0-44a6-b83e-d278248a1200",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2031\u001B[0m, in \u001B[0;36mRow.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   2028\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   2029\u001B[0m     \u001B[38;5;66;03m# it will be slow when it has many fields,\u001B[39;00m\n",
       "\u001B[1;32m   2030\u001B[0m     \u001B[38;5;66;03m# but this will not be used in normal cases\u001B[39;00m\n",
       "\u001B[0;32m-> 2031\u001B[0m     idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__fields__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   2032\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(Row, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(idx)\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: (0, 3) is not in list\n",
       "\n",
       "During handling of the above exception, another exception occurred:\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-3270515310027764>:1\u001B[0m\n",
       "\u001B[0;32m----> 1\u001B[0m \u001B[43mrow1\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2036\u001B[0m, in \u001B[0;36mRow.__getitem__\u001B[0;34m(self, item)\u001B[0m\n",
       "\u001B[1;32m   2034\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(item)\n",
       "\u001B[1;32m   2035\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n",
       "\u001B[0;32m-> 2036\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(item)\n",
       "\n",
       "\u001B[0;31mValueError\u001B[0m: (0, 3)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2031\u001B[0m, in \u001B[0;36mRow.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2028\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   2029\u001B[0m     \u001B[38;5;66;03m# it will be slow when it has many fields,\u001B[39;00m\n\u001B[1;32m   2030\u001B[0m     \u001B[38;5;66;03m# but this will not be used in normal cases\u001B[39;00m\n\u001B[0;32m-> 2031\u001B[0m     idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__fields__\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m(\u001B[49m\u001B[43mitem\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2032\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m(Row, \u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__getitem__\u001B[39m(idx)\n\n\u001B[0;31mValueError\u001B[0m: (0, 3) is not in list\n\nDuring handling of the above exception, another exception occurred:\n\n\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)\nFile \u001B[0;32m<command-3270515310027764>:1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mrow1\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/types.py:2036\u001B[0m, in \u001B[0;36mRow.__getitem__\u001B[0;34m(self, item)\u001B[0m\n\u001B[1;32m   2034\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(item)\n\u001B[1;32m   2035\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m:\n\u001B[0;32m-> 2036\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(item)\n\n\u001B[0;31mValueError\u001B[0m: (0, 3)",
       "errorSummary": "<span class='ansi-red-fg'>ValueError</span>: (0, 3)",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "row1[0][0,3]"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_column_operations",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}